{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3356b91-6e36-4894-9d43-4db650b566da",
   "metadata": {},
   "source": [
    "# Application of LLM for our own tasks\n",
    "We can take advantage of previously developed LLM and optimize it for our own specific tasks, and this way we can save our time and resource significantly. When doing so, we still want to fine-tune the parameters so the LLM can be further optimized. There are many techniques to do that. In this exercise we will try two methods : \n",
    "1. Updating all the layers\n",
    "2. Updating only the final layers\n",
    "\n",
    "This notebook is shows an example of how to use a LLM model for a specific task of classifying spam SMS messages. We will use two different fine-tuning methods applied to the previously trained DistilBERT model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1d0be-5fff-405c-9549-47923cb077d3",
   "metadata": {},
   "source": [
    "## 1. First prepare for SMS Spam dataset from the Hugging Face Transformers libarary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510837fe-9573-4114-a420-da82e81a32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# SMS Spam dataset has only train so we need to create test dataset out of the train set.\n",
    "sms_dataset = load_dataset(\"sms_spam\", split='train').train_test_split(test_size=0.2, shuffle=True, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4909c7-c015-41c6-bd8d-a1d251af4047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sms', 'label'],\n",
      "    num_rows: 4459\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sms', 'label'],\n",
      "    num_rows: 1115\n",
      "})\n",
      "{'sms': 'The monthly amount is not that terrible and you will not pay anything till 6months after finishing school.\\n', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'test']\n",
    "\n",
    "print(sms_dataset['train'])\n",
    "print(sms_dataset['test'])\n",
    "print(sms_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7220063-570f-4712-b026-053155f9a788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1115\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# We will use DistilBERT LLM model to tokenize the data in the dataset\n",
    "sms_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = sms_dataset[split].map(lambda x: sms_tokenizer(x['sms'], truncation=True), batched=True)\n",
    "\n",
    "tokenized_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cba0f4-c3c3-4d5a-a488-5e76afaa4195",
   "metadata": {},
   "source": [
    "## 2. Set up a model that would allow updating all parameters in all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9e46f9-7cdc-4dfc-a70c-55a7b730f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# First we will start with the parameters that were trained previously. We are attaching a fully-connected layer to classify \"It's a Spam\" vs \"Not a Spam\".\n",
    "# You will get a warning message that says something like some weights are not initialized because of the added classification layer.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           num_labels=2, \n",
    "                                                           id2label={0:\"Not a Spam\", 1:\" It's a Spam\"},\n",
    "                                                           label2id={\"Not a Spam\":0, \"It's a Spam\":1})\n",
    "\n",
    "# We will enable updating of all parameters in this model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=True\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102ee08-a0b0-4994-a0a7-efd34411837a",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "Trainer class will make training easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4b51d2-959c-4004-80aa-bfb7be39eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkang\\AppData\\Local\\Temp\\ipykernel_15188\\333892526.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model = model,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions==labels).mean()}\n",
    "\n",
    "trainer = Trainer(model = model,\n",
    "                  args=TrainingArguments(output_dir=\"./data/sms_spam\",\n",
    "                                         learning_rate=2e-5,\n",
    "                                         per_device_train_batch_size=64,\n",
    "                                         per_device_eval_batch_size=64,\n",
    "                                         eval_strategy=\"epoch\",\n",
    "                                         save_strategy=\"epoch\",\n",
    "                                         num_train_epochs=10,\n",
    "                                         weight_decay=0.01,\n",
    "                                         load_best_model_at_end=True),\n",
    "                  train_dataset=tokenized_dataset['train'],\n",
    "                  eval_dataset=tokenized_dataset['test'],\n",
    "                  tokenizer=sms_tokenizer,\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=sms_tokenizer),\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd479f06-85e6-422d-bf25-13f2b8708dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/06 23:56:59 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 01:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.991928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.994619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>0.994619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029790</td>\n",
       "      <td>0.992825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.032537</td>\n",
       "      <td>0.994619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.994619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.031381</td>\n",
       "      <td>0.993722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.032130</td>\n",
       "      <td>0.993722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.994619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 100.6676 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# We will calculate the time spent for training\n",
    "start_time = time.perf_counter()\n",
    "trainer.train()\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97272994-5814-4d14-bfa5-acdd32449acf",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fbbdd1-2543-405b-a31d-fac52873d457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.024361399933695793,\n",
       " 'eval_accuracy': 0.9946188340807175,\n",
       " 'eval_runtime': 1.0806,\n",
       " 'eval_samples_per_second': 1031.878,\n",
       " 'eval_steps_per_second': 16.658,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27368150-966b-4854-98c0-599a9ec0fe56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And miss vday the parachute and double coins??...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tmrw. Im finishing 9 doors\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No it will reach by 9 only. She telling she wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thanks love. But am i doing torch or bold.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cool. Do you like swimming? I have a pool and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FREE for 1st week! No1 Nokia tone 4 ur mob eve...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2 celebrate my bday, y else?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sms  Predictions  labels\n",
       "0  And miss vday the parachute and double coins??...            0       0\n",
       "1                       Tmrw. Im finishing 9 doors\\n            0       0\n",
       "2  No it will reach by 9 only. She telling she wi...            0       0\n",
       "3       Thanks love. But am i doing torch or bold.\\n            0       0\n",
       "4  Cool. Do you like swimming? I have a pool and ...            0       0\n",
       "5  FREE for 1st week! No1 Nokia tone 4 ur mob eve...            1       1\n",
       "6                    2 celebrate my bday, y else?\\n            0       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "items_for_check = tokenized_dataset['test'].select([32, 434, 51, 900, 234, 124, 2])\n",
    "\n",
    "results = trainer.predict(items_for_check)\n",
    "df = pd.DataFrame({\"sms\": [item[\"sms\"] for item in items_for_check],\n",
    "                   \"Predictions\": results.predictions.argmax(axis=1),\n",
    "                   \"labels\": results.label_ids})\n",
    "                   \n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b96ddd-5734-4c05-b9a3-dfbbe6dff54c",
   "metadata": {},
   "source": [
    "### If we update all parameters, we start from 97% and end with 99%. And we spent about 100s for fine-tuning the parameters. Now let's just update the final classification and pre-classification layers while freezing parameter updates on all the other layers. This way we will save some time on training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05593d3d-bebf-4da6-8cb8-5297e0ea938e",
   "metadata": {},
   "source": [
    "## 5. Now try only updating smaller set of parameters (only at the final classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e45957-194e-4d8f-b019-9a5a75577e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jkang\\AppData\\Local\\Temp\\ipykernel_15188\\3956421398.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_update_only_classifier = Trainer(model = model_update_only_classifier,\n"
     ]
    }
   ],
   "source": [
    "# Now create a fresh model that has the original pretrained parameters.\n",
    "model_update_only_classifier = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           num_labels=2, \n",
    "                                                           id2label={0:\"Not a Saapam\", 1:\" It's a Spam\"},\n",
    "                                                           label2id={\"Not a Spam\":0, \"It's a Spam\":1})\n",
    "\n",
    "# Let's freeze parameter updates in all layers except for the final classification layer that we added for this specific task.\n",
    "for param in model_update_only_classifier.parameters():\n",
    "    param.requires_grad=False\n",
    "for param in model_update_only_classifier.pre_classifier.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model_update_only_classifier.classifier.parameters():\n",
    "    param.requires_grad=True\n",
    "\n",
    "trainer_update_only_classifier = Trainer(model = model_update_only_classifier,\n",
    "                  args=TrainingArguments(output_dir=\"./data/sms_spam\",\n",
    "                                         learning_rate=2e-5,\n",
    "                                         per_device_train_batch_size=64,\n",
    "                                         per_device_eval_batch_size=64,\n",
    "                                         eval_strategy=\"epoch\",\n",
    "                                         save_strategy=\"epoch\",\n",
    "                                         num_train_epochs=10,\n",
    "                                         weight_decay=0.01,\n",
    "                                         load_best_model_at_end=True),\n",
    "                  train_dataset=tokenized_dataset['train'],\n",
    "                  eval_dataset=tokenized_dataset['test'],\n",
    "                  tokenizer=sms_tokenizer,\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=sms_tokenizer),\n",
    "\n",
    "                                         compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2f9775-12a9-47b5-a29a-b73e8639ea67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 00:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.356493</td>\n",
       "      <td>0.852018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.266850</td>\n",
       "      <td>0.852018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197318</td>\n",
       "      <td>0.916592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.157021</td>\n",
       "      <td>0.943498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.130478</td>\n",
       "      <td>0.968610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.973094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105438</td>\n",
       "      <td>0.978475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.099316</td>\n",
       "      <td>0.979372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.095866</td>\n",
       "      <td>0.980269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>0.980269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time without updating all parameters : 42.9449 seconds\n"
     ]
    }
   ],
   "source": [
    "# We will calculate the time spent for training\n",
    "start_time = time.perf_counter()\n",
    "trainer_update_only_classifier.train()\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time without updating all parameters : {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a4a8c-1246-4202-a782-12f39bb91914",
   "metadata": {},
   "source": [
    "### If we only update the final 2 layers, we can save the training time by about 57% while we still managed to get decent results. 99.5% vs 98%. But fine tuning all layers seems to reach 99.5% after the 3rd epoch while updating partial layers take about 9 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cc420-1e58-4b81-94bc-34a458075064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
